{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same list of stopwords as the authors of the paper.\n",
    "# This list doesn't exactly match sklearn's.\n",
    "STOP_WORDS = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "    \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
    "    \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
    "    \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
    "    \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
    "    \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
    "    \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "    \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "    \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "    \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
    "    \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
    "    \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "    \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "    \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "    \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
    "    \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "    \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
    "    \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
    "    \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "    \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
    "    \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]\n",
    "\n",
    "VOCABULARY_SIZE = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to pull data from the disk into memory.\n",
    "# Caveats: some headers are defined for multiple labels. This code will\n",
    "# ensure that only one instance for each body and header is kept in memory.\n",
    "\n",
    "\n",
    "def get_or_put(list_, value):\n",
    "    \"\"\"Inserts value in list_ if not preset, returns the index of value.\"\"\"\n",
    "    try:\n",
    "        return list_.index(value)\n",
    "    except ValueError:\n",
    "        list_.append(value)\n",
    "        return len(list_) - 1\n",
    "\n",
    "    \n",
    "def read_stance(filepath, has_labels=True):\n",
    "    \"\"\"Reads the stances present on the filepath csv file.\n",
    "    \n",
    "    :return (a,b) where a is a list of dicts that describe each sample and b\n",
    "    is a list of headlines. The body_id on the elements of a correspond to the\n",
    "    text on the i-th element of b.\n",
    "    \"\"\"\n",
    "    samples = list()\n",
    "    headlines = list()\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for line in reader:\n",
    "            headline = line['Headline']\n",
    "            hid = get_or_put(headlines, headline)\n",
    "            \n",
    "            node = {\n",
    "                'headline': hid,\n",
    "                'body_id': int(line['Body ID']),                \n",
    "            }\n",
    "            \n",
    "            if has_labels:\n",
    "                node['label'] = line['Stance']\n",
    "            \n",
    "            samples.append(node)\n",
    "            \n",
    "    return samples, headlines\n",
    "\n",
    "\n",
    "def read_bodies(filepath):\n",
    "    \"\"\"Produces a dict mapping body ids to the text on the filepath csv file.\"\"\"\n",
    "    ordered_bodies = list()\n",
    "    bodies_index = dict()\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for line in reader:\n",
    "            body_id = int(line['Body ID'])\n",
    "            body = line['articleBody']\n",
    "\n",
    "            if body_id not in bodies_index:\n",
    "                ordered_bodies.append(body)\n",
    "                bodies_index[body_id] = len(ordered_bodies) - 1\n",
    "                \n",
    "    return ordered_bodies, bodies_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data statistics for train split:\n",
      " - Amount of samples:   49972\n",
      " - Amount of headlines: 1648\n",
      " - Amount of bodies:    1683\n",
      "\n",
      "Data statistics for test split:\n",
      " - Amount of samples:   25413\n",
      " - Amount of headlines: 894\n",
      " - Amount of bodies:    904\n"
     ]
    }
   ],
   "source": [
    "# Loading training data\n",
    "\n",
    "def show_statistics(amount_samples, amount_heads, amount_bodies, fold):\n",
    "    print('Data statistics for {} split:'.format(fold))\n",
    "    print(' - Amount of samples:   {}'.format(amount_samples))\n",
    "    print(' - Amount of headlines: {}'.format(amount_heads))\n",
    "    print(' - Amount of bodies:    {}'.format(amount_bodies))\n",
    "\n",
    "\n",
    "train_samples, train_headlines = read_stance('../fakenewschallenge/train_stances.csv')\n",
    "train_bodies, train_bodies_map = read_bodies('../fakenewschallenge/train_bodies.csv')\n",
    "\n",
    "test_samples, test_headlines = read_stance('../fakenewschallenge/test_stances_unlabeled.csv', False)\n",
    "test_bodies, test_bodies_map = read_bodies('../fakenewschallenge/test_bodies.csv')\n",
    "\n",
    "print()\n",
    "show_statistics(len(train_samples), len(train_headlines), len(train_bodies), 'train')\n",
    "print()\n",
    "show_statistics(len(test_samples), len(test_headlines), len(test_bodies), 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and transforming TF vectorizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lzfelix/anaconda3/envs/mirror/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and transforming TF-IDF vectorizer.\n"
     ]
    }
   ],
   "source": [
    "# Fitting vectorizers to build feature vectors for each sample\n",
    "\n",
    "print('Fitting and transforming TF vectorizer.')\n",
    "all_train_texts = train_headlines + train_bodies\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_features=VOCABULARY_SIZE, stop_words=STOP_WORDS, use_idf=False)\n",
    "all_tfs = tf_vectorizer.fit_transform(all_train_texts)\n",
    "\n",
    "# WARN: Using test data for training LOL. But I'm just replicating the paper's implementation\n",
    "print('Fitting and transforming TF-IDF vectorizer.')\n",
    "all_texts = all_train_texts + test_headlines + test_bodies\n",
    "tfidf_transformer = TfidfVectorizer(max_features=VOCABULARY_SIZE, stop_words=STOP_WORDS)\n",
    "_ = tfidf_transformer.fit(all_texts)\n",
    "\n",
    "# Points to the first headline on the list of TFs computed when fitting the vectorizer.\n",
    "first_headline = len(train_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lzfelix/anaconda3/envs/mirror/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_vector(tf_head, tf_body, similarity):        \n",
    "    return np.concatenate((tf_head.toarray(), tf_body.toarray(), similarity), axis=1)[0]\n",
    "\n",
    "\n",
    "# Building the feature vectors for samples\n",
    "feature_samples = list()\n",
    "for y, sample in enumerate(train_samples):\n",
    "    headline_id = sample['headline']\n",
    "    body_id     = sample['body_id']\n",
    "    \n",
    "    body_index = train_bodies_map[body_id]\n",
    "\n",
    "    tf_headline = all_tfs[headline_id]\n",
    "    tf_body     = all_tfs[first_headline + body_index]\n",
    "    \n",
    "    tf_idf_head = tfidf_transformer.transform([train_headlines[headline_id]])\n",
    "    tf_idf_body = tfidf_transformer.transform([train_bodies[body_index]])\n",
    "    \n",
    "    s = cosine_similarity(tf_idf_head, tf_idf_body, dense_output=True)\n",
    "    \n",
    "    feature_samples.append(build_vector(tf_headline, tf_body, s))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mirror]",
   "language": "python",
   "name": "conda-env-mirror-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
